{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "import dynaphos\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "from dynaphos.simulator import GaussianSimulator as PhospheneSimulator\n",
    "from dynaphos import cortex_models\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from utils import resize\n",
    "\n",
    "import local_datasets\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "from utils import (\n",
    "    CustomSummaryTracker,\n",
    "    dilation3x3,\n",
    "    resize,\n",
    "    tensor_to_rgb,\n",
    "    undo_standardize,\n",
    ")\n",
    "\n",
    "import importlib\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'local_datasets' from '/gpfs/home1/mappel/Dynaphos/viseon/local_datasets.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(local_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(yaml_file):\n",
    "    with open(yaml_file) as file:\n",
    "        raw_content = yaml.load(file,Loader=yaml.FullLoader) # nested dictionary\n",
    "    return {k:v for params in raw_content.values() for k,v in params.items()} # unpacked\n",
    "\n",
    "cfg = load_config('../_config/exp-local/faces_boundary_supervised_phosphenes_no_decoder.yaml')\n",
    "cfg['retinal_compression'] = True\n",
    "cfg['data_directory'] = '../../data/face/LaPa'\n",
    "cfg['base_config'] = '../_config/exp-local/params.yaml'\n",
    "cfg['phosphene_map'] = '../_config/phosphene_maps/DefaultCoordinateMap_1000_phosphenes.pickle'\n",
    "cfg['batch_size'] = 1\n",
    "cfg['fov'] = 5\n",
    "cfg['resolution'] = [256, 256]\n",
    "cfg['output_scaling'] = 1.e-4\n",
    "cfg['n_electrodes'] = 1024\n",
    "params = dynaphos.utils.load_params(cfg['base_config'])\n",
    "params['run']['batch_size'] = cfg['batch_size']\n",
    "\n",
    "cfg['retinal_compression'] = True\n",
    "cfg['imsize'] = [64, 64]\n",
    "cfg['fov'] = 16\n",
    "cfg['circular_mask'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset = local_datasets.get_lapa_dataset(cfg)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=cfg['batch_size'], shuffle=True, drop_last=True)\n",
    "valloader = DataLoader(valset, batch_size=cfg['batch_size'], shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_batch = next(iter(valloader))\n",
    "\n",
    "plt.imshow(example_batch['image'][0].permute(1,2,0).cpu())\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_simulator(cfg, phosphene_coords=None):\n",
    "    # initialise simulator\n",
    "\n",
    "    params = dynaphos.utils.load_params(cfg['base_config'])\n",
    "    params['run']['batch_size'] = cfg['batch_size']\n",
    "\n",
    "    if phosphene_coords is None:\n",
    "        n_phosphenes = cfg['n_electrodes']\n",
    "        phosphene_coords = cortex_models.get_visual_field_coordinates_probabilistically(params, n_phosphenes)\n",
    "\n",
    "    simulator = PhospheneSimulator(params, phosphene_coords)\n",
    "    return simulator\n",
    "\n",
    "n_phosphenes = cfg['n_electrodes']\n",
    "params = dynaphos.utils.load_params(cfg['base_config'])\n",
    "params['batch_size'] = 4\n",
    "phosphene_coords = cortex_models.get_visual_field_coordinates_probabilistically(params, n_phosphenes)\n",
    "simulator = get_simulator(cfg, phosphene_coords=phosphene_coords)\n",
    "cfg['SPVsize'] = simulator.phosphene_maps.shape[-2:]\n",
    "len(simulator.phosphene_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phosphene map size: torch.Size([256, 256])\n",
      "Number of phosphenes: 1024\n"
     ]
    }
   ],
   "source": [
    "cfg['SPVsize'] = simulator.phosphene_maps.shape[-2:]\n",
    "print(f\"Phosphene map size: {cfg['SPVsize']}\")\n",
    "N_PHOSPHENES = simulator.num_phosphenes\n",
    "print(f\"Number of phosphenes: {N_PHOSPHENES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = phosphene_coords._x, phosphene_coords._y\n",
    "coords = torch.Tensor(np.array([x, y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape_square = np.sqrt(N_PHOSPHENES)\n",
    "\n",
    "# if the sqrt is not an integer, we need to find the closest integer that is bigger\n",
    "if int(reshape_square) != reshape_square:\n",
    "    reshape_square = int(reshape_square) + 1\n",
    "else:\n",
    "    reshape_square = int(reshape_square)\n",
    "    \n",
    "reshape_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2f662e07f14a34a458f2f3bbac8c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='Shift X', max=0.5, min=-0.5, step=0.001), FloatSlideâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to update the plot based on the slider values\n",
    "def update_plot(shift_x=0.5, shift_y=0.5, scale=20):\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    shifted_image = torch.roll(face_image, int(shift_x * face_image.shape[-1]), dims=-1)\n",
    "    shifted_image = torch.roll(shifted_image, int(shift_y * face_image.shape[-2]), dims=-2)\n",
    "    \n",
    "    # scale content and fit into original image size\n",
    "    if scale > 0 and scale < 1.:\n",
    "        scaled_image = F.interpolate(shifted_image, scale_factor=scale, mode='nearest')\n",
    "        shifted_image = torch.zeros((1, 1, *cfg['imsize']), device=cfg['device'])\n",
    "        shifted_image[0, 0, \n",
    "                    int((cfg['imsize'][0] - scaled_image.shape[-2]) / 2):int((cfg['imsize'][0] + scaled_image.shape[-2]) / 2), \n",
    "                    int((cfg['imsize'][1] - scaled_image.shape[-1]) / 2):int((cfg['imsize'][1] + scaled_image.shape[-1]) / 2)] = scaled_image\n",
    "    elif scale == 0.: # no scaling\n",
    "        shifted_image = torch.zeros((1, 1, *cfg['imsize']), device=cfg['device'])\n",
    "    elif scale > 1.:\n",
    "        scaled_image = F.interpolate(shifted_image, scale_factor=scale, mode='nearest')\n",
    "        # crop\n",
    "        shifted_image = scaled_image[:, :,\n",
    "                    int((scaled_image.shape[-2] - cfg['imsize'][0]) / 2):int((scaled_image.shape[-2] + cfg['imsize'][0]) / 2),\n",
    "                    int((scaled_image.shape[-1] - cfg['imsize'][1]) / 2):int((scaled_image.shape[-1] + cfg['imsize'][1]) / 2)]\n",
    "        \n",
    "    # Update the visual input display\n",
    "    ax[0].imshow(shifted_image[0].permute(1,2,0).cpu().numpy(), cmap='gray')\n",
    "    ax[0].set_title('Visual Input')\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    # Update the phosphene display\n",
    "    resized_image = resize(shifted_image, cfg['SPVsize'], interpolation='nearest')\n",
    "    stimulation = simulator.sample_centers(resized_image.mean(1, keepdim=True)) * cfg['output_scaling']\n",
    "    simulator.reset()\n",
    "    phosphenes = simulator(stimulation).unsqueeze(1)\n",
    "    \n",
    "    ax[1].imshow(phosphenes.squeeze().detach().cpu().numpy(), cmap='gray')\n",
    "    ax[1].set_title('Phosphenes')\n",
    "    ax[1].axis('off')\n",
    "    \n",
    "    # Update the stimulation display\n",
    "    stimulation_square = torch.zeros((reshape_square, reshape_square), device=cfg['device'])\n",
    "    stimulation_square.view(-1)[:N_PHOSPHENES] = stimulation\n",
    "\n",
    "    ax[2].imshow(stimulation_square.cpu().numpy(), cmap='gray')\n",
    "    ax[2].set_title('Stimulation')\n",
    "    ax[2].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate the circle image with the specified size\n",
    "face_image = example_batch['contour'].to(cfg['device'])\n",
    "face_image = dilation3x3(face_image)\n",
    "\n",
    "# Display the interactive widgets\n",
    "plot = interact(update_plot, \n",
    "         shift_x=FloatSlider(min=-0.5, max=0.5, step=0.001, value=0., description='Shift X'), \n",
    "         shift_y=FloatSlider(min=-0.5, max=0.5, step=0.001, value=0., description='Shift Y'),\n",
    "         scale=FloatSlider(min=0.0, max=3.0, step=0.001, value=1., description='Scale'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retinal Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 96\u001b[0m\n\u001b[1;32m     93\u001b[0m torch\u001b[38;5;241m.\u001b[39muse_deterministic_algorithms(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# test forward pass\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mretinal_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout size:\u001b[39m\u001b[38;5;124m'\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mmin(), output\u001b[38;5;241m.\u001b[39mmax(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 88\u001b[0m, in \u001b[0;36mRetinal_Encoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 88\u001b[0m     stimulation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stimulation\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_scaling\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/functional.py:2507\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2495\u001b[0m         batch_norm,\n\u001b[1;32m   2496\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2504\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2505\u001b[0m     )\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2507\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2510\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2511\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/ML/lib/python3.8/site-packages/torch/nn/functional.py:2475\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2473\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 512, 1, 1])"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, n_channels, stride=1, resample_out=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(n_channels, n_channels,kernel_size=3, stride=1,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(n_channels)\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(n_channels, n_channels,kernel_size=3, stride=1,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(n_channels)\n",
    "        self.resample_out = resample_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        if self.resample_out:\n",
    "            out = self.resample_out(out)\n",
    "        return out\n",
    "    \n",
    "def convlayer(n_input, n_output, k_size=3, stride=1, padding=1, resample_out=None):\n",
    "    layer = [\n",
    "        nn.Conv2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=False),\n",
    "        nn.BatchNorm2d(n_output),\n",
    "        nn.LeakyReLU(inplace=True),\n",
    "        resample_out]\n",
    "    if resample_out is None:\n",
    "        layer.pop()\n",
    "    return layer\n",
    "\n",
    "class Retinal_Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1, n_electrodes=1024, out_scaling=1e-4, out_activation='relu'):\n",
    "        super(Retinal_Encoder, self).__init__()\n",
    "        self.output_scaling = out_scaling\n",
    "        self.out_activation = {'tanh': nn.Tanh(), ## NOTE: simulator expects only positive stimulation values \n",
    "                               'sigmoid': nn.Sigmoid(),\n",
    "                               'relu': nn.ReLU(),\n",
    "                               'softmax':nn.Softmax(dim=1)}[out_activation]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "                                    *convlayer(in_channels,8,3,1,1),\n",
    "                                    *convlayer(8,16,3,2,1),\n",
    "                                    *convlayer(16,32,3,2,1),\n",
    "                                    *convlayer(32,64,3,2,1),\n",
    "                                    *convlayer(64,128,3,2,1),\n",
    "                                    *convlayer(128,256,3,2,1),\n",
    "                                    *convlayer(256,512,3,2,1),\n",
    "                                    *convlayer(512,1024,3,1,1),\n",
    "                                    nn.AvgPool2d(2),\n",
    "                                    self.out_activation\n",
    "                                   )\n",
    "        \n",
    "        \n",
    "        # self.model = nn.Sequential(\n",
    "        #                             *convlayer(in_channels,8,3,2,1),\n",
    "        #                             # *convlayer(8,16,3,2,1),\n",
    "        #                             *convlayer(8,32,3,2,1),\n",
    "        #                             # *convlayer(32,64,3,2,1),\n",
    "        #                             *convlayer(32,128,3,2,1),\n",
    "        #                             # *convlayer(128,256,3,2,1),\n",
    "        #                             *convlayer(128,512,3,2,1),\n",
    "        #                             *convlayer(512,1024,3,2,1),\n",
    "        #                             nn.AvgPool2d(2),\n",
    "        #                             self.out_activation\n",
    "        #                            )\n",
    "\n",
    "        # self.model = nn.Sequential(*convlayer(in_channels,8,3,1,1),\n",
    "        #                            *convlayer(8,16,3,1,1,resample_out=nn.MaxPool2d(2)),\n",
    "        #                            *convlayer(16,32,3,1,1,resample_out=nn.MaxPool2d(2)),\n",
    "        #                            ResidualBlock(32, resample_out=None),\n",
    "        #                            ResidualBlock(32, resample_out=None),\n",
    "        #                            ResidualBlock(32, resample_out=None),\n",
    "        #                            ResidualBlock(32, resample_out=None),\n",
    "        #                            *convlayer(32,16,3,1,1),\n",
    "        #                            nn.Conv2d(16,1,3,1,1),\n",
    "        #                            nn.Flatten(),\n",
    "        #                            nn.Linear(256,n_electrodes),\n",
    "        #                            self.out_activation)\n",
    "            \n",
    "        \n",
    "        self.n_electrodes = n_electrodes\n",
    "        self.fc = None\n",
    "            \n",
    "    def forward(self, x):\n",
    "        stimulation = self.model(x)\n",
    "        return stimulation.squeeze(-1).squeeze(-1) * self.output_scaling\n",
    "\n",
    "\n",
    "retinal_encoder = Retinal_Encoder(n_electrodes=cfg['n_electrodes'], out_scaling=cfg['output_scaling'], out_activation='relu').to(cfg['device'])\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "# test forward pass\n",
    "output = retinal_encoder(torch.randn(1, 3, 64, 64).to(cfg['device']))\n",
    "\n",
    "print('out size:', output.shape)\n",
    "print(output.min(), output.max(), '\\n')\n",
    "\n",
    "# count number of parameters\n",
    "print('Retinal Encoder Parameters:', sum(p.numel() for p in retinal_encoder.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: torch.Size([1, 3, 64, 64])\n",
      "1.62 ms Â± 71 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n",
      "0.9813308715820312 ms\n",
      "Retinal Encoder Parameters: 5351208\n"
     ]
    }
   ],
   "source": [
    "retinal_encoder = Retinal_Encoder(n_electrodes=cfg['n_electrodes'], out_scaling=cfg['output_scaling'], out_activation='relu').to(cfg['device'])\n",
    "\n",
    "input = torch.randn(1, 3, 64, 64).to(cfg['device'])\n",
    "\n",
    "# Warm up the model (optional but recommended for more accurate timing)\n",
    "for _ in range(10):\n",
    "    _ = retinal_encoder(input)\n",
    "\n",
    "def forward_pass():\n",
    "    torch.cuda.synchronize()\n",
    "    _ = retinal_encoder(input)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# time test \n",
    "\n",
    "start = time()\n",
    "retinal_encoder(input)\n",
    "end = time()\n",
    "\n",
    "# print the input size, network architecture, time taken for forward pass, and number of parameters\n",
    "print('Input size:', input.shape)\n",
    "# print(retinal_encoder)\n",
    "%timeit forward_pass()\n",
    "print((end - start) * 1000, 'ms')\n",
    "print('Retinal Encoder Parameters:', sum(p.numel() for p in retinal_encoder.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaap's encoder\n",
    "Input size: torch.Size([1, 3, 128, 128]) \\\n",
    "3.72 ms Â± 328 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each) \\\n",
    "5.002737045288086 ms \\\n",
    "Retinal Encoder Parameters: 1134969\n",
    "\n",
    "Input size: torch.Size([1, 3, 64, 64]) \\\n",
    "4.11 ms Â± 499 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each) \\\n",
    "3.983736038208008 ms \\\n",
    "Retinal Encoder Parameters: 348537\n",
    "\n",
    "### Fully conv encoder\n",
    "Input size: torch.Size([1, 3, 128, 128]) \\\n",
    "2.7 ms Â± 293 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each) \\\n",
    "2.0041465759277344 ms \\\n",
    "Retinal Encoder Parameters: 6295368\n",
    "\n",
    "Input size: torch.Size([1, 3, 64, 64]) \\\n",
    "1.62 ms Â± 71 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each) \\\n",
    "0.9813308715820312 ms \\\n",
    "Retinal Encoder Parameters: 5351208\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small round of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 4.00 GiB total capacity; 3.29 GiB already allocated; 0 bytes free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [456], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m retinal_encoder\u001b[38;5;241m.\u001b[39mto(cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m phosphene_coords \u001b[38;5;241m=\u001b[39m cortex_models\u001b[38;5;241m.\u001b[39mget_visual_field_coordinates_probabilistically(params, n_phosphenes)\n\u001b[1;32m---> 11\u001b[0m simulator \u001b[38;5;241m=\u001b[39m \u001b[43mget_simulator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphosphene_coords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Define loss function\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss()\n",
      "Cell \u001b[1;32mIn [332], line 11\u001b[0m, in \u001b[0;36mget_simulator\u001b[1;34m(cfg, phosphene_coords)\u001b[0m\n\u001b[0;32m      8\u001b[0m     n_phosphenes \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_electrodes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m     phosphene_coords \u001b[38;5;241m=\u001b[39m cortex_models\u001b[38;5;241m.\u001b[39mget_visual_field_coordinates_probabilistically(params, n_phosphenes)\n\u001b[1;32m---> 11\u001b[0m simulator \u001b[38;5;241m=\u001b[39m \u001b[43mPhospheneSimulator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphosphene_coords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m simulator\n",
      "File \u001b[1;32mc:\\Users\\appel\\anaconda3\\envs\\ML\\lib\\site-packages\\dynaphos\\simulator.py:188\u001b[0m, in \u001b[0;36mGaussianSimulator.__init__\u001b[1;34m(self, params, coordinates, rng, theta)\u001b[0m\n\u001b[0;32m    183\u001b[0m set_deterministic(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeg2pix_coeff \u001b[38;5;241m=\u001b[39m get_deg2pix_coeff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphosphene_maps \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m--> 188\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_phosphene_maps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoordinates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\appel\\anaconda3\\envs\\ML\\lib\\site-packages\\dynaphos\\simulator.py:307\u001b[0m, in \u001b[0;36mGaussianSimulator.generate_phosphene_maps\u001b[1;34m(self, coordinates, remove_invalid, theta)\u001b[0m\n\u001b[0;32m    305\u001b[0m     phosphene_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgabor_rotation(x, y, theta)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 307\u001b[0m     phosphene_maps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m)\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m phosphene_maps\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 4.00 GiB total capacity; 3.29 GiB already allocated; 0 bytes free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "cfg['batch_size'] = 2\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=cfg['batch_size'], shuffle=True, drop_last=True)\n",
    "\n",
    "# Initialize the retinal encoder\n",
    "cfg['output_scaling'] = 0.0002\n",
    "retinal_encoder = Retinal_Encoder(out_activation='relu', in_channels=3, n_electrodes=cfg['n_electrodes'], out_scaling=cfg['output_scaling'])\n",
    "retinal_encoder.to(cfg['device'])\n",
    "\n",
    "phosphene_coords = cortex_models.get_visual_field_coordinates_probabilistically(params, n_phosphenes)\n",
    "simulator = get_simulator(cfg, phosphene_coords)\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = nn.L1Loss()\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(retinal_encoder.parameters(), lr=0.0009)\n",
    "\n",
    "# Set deterministic algorithms off\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "for i in range(800):\n",
    "    # Generate random image and make non differentiable\n",
    "    batch = next(iter(trainloader))\n",
    "    random_image, contour = batch['image'].to(cfg['device']), batch['contour'].to(cfg['device'])\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    stimulation = retinal_encoder(random_image)\n",
    "    simulator.reset()\n",
    "    phosphenes = simulator(stimulation)\n",
    "    simulator.reset()\n",
    "    phosphene_centers = simulator.sample_centers(phosphenes)\n",
    "    \n",
    "    \n",
    "    # Define target\n",
    "    resized = resize(contour.float(), cfg['SPVsize'])\n",
    "    simulator.reset()\n",
    "    target = simulator.sample_centers(resized.mean(1, keepdim=True))\n",
    "    simulator.reset()\n",
    "    target_stimulus = simulator.sample_stimulus(resized.mean(1, keepdim=True)*cfg['output_scaling'])\n",
    "\n",
    "    # Calculate loss\n",
    "    # loss = loss_fn(phosphene_centers, target)\n",
    "    loss = loss_fn(stimulation, target_stimulus)\n",
    "\n",
    "    # Zero gradients, backward pass, and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 25 == 0:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # print(f'Epoch {i} Loss: {loss.item()}')\n",
    "        # show input image and phosphenes, and combined/overlayed image, target\n",
    "\n",
    "        fig, ax = plt.subplots(2, 4, figsize=(22, 12))\n",
    "        ax[0, 0].imshow(random_image[0].permute(1, 2, 0).detach().cpu().numpy(), cmap='gray')\n",
    "        ax[0, 0].axis('off')\n",
    "        ax[0, 0].set_title('Visual Input')\n",
    "\n",
    "        # phosphene image\n",
    "        ax[0, 1].imshow(phosphenes[0].detach().cpu().numpy(), cmap='gray')\n",
    "        ax[0, 1].axis('off')\n",
    "        ax[0, 1].set_title('Phosphenes')\n",
    "\n",
    "        # combined image\n",
    "        ax[0, 2].imshow(resized[0].permute(1, 2, 0).detach().cpu().numpy(), cmap='gray')\n",
    "        ax[0, 2].imshow(phosphenes[0].detach().cpu().numpy(), cmap='gray', alpha=0.5)\n",
    "        ax[0, 2].axis('off')\n",
    "        ax[0, 2].set_title('Combined Image')\n",
    "    \n",
    "        # target\n",
    "        simulator.reset()\n",
    "        out = simulator(target_stimulus)[0]\n",
    "        ax[0, 3].imshow(out.detach().cpu().numpy(), cmap='gray')\n",
    "        ax[0, 3].axis('off')\n",
    "        ax[0, 3].set_title('Target')\n",
    "\n",
    "        # 1,0 text block with training stats\n",
    "        ax[1, 0].text(0.1, 0.5, f'Epoch {i} Loss: {loss.item()}', fontsize=12, ha='left')\n",
    "        ax[1, 0].text(0.1, 0.4, f'Max Stimul: {stimulation.max().item()}', fontsize=12, ha='left')\n",
    "        ax[1, 0].text(0.1, 0.3, f'Max Target: {target_stimulus.max().item()}', fontsize=12, ha='left')\n",
    "        ax[1, 0].axis('off')\n",
    "        \n",
    "        # Create a square stimulation grid with a mask\n",
    "        stimulation_square = torch.zeros((reshape_square, reshape_square), device=cfg['device'])\n",
    "        stimulation_square.view(-1)[:N_PHOSPHENES] = stimulation[0]\n",
    "\n",
    "        stimulation_target_square = torch.zeros((reshape_square, reshape_square), device=cfg['device'])\n",
    "        stimulation_target_square.view(-1)[:N_PHOSPHENES] = target[0]\n",
    "\n",
    "        ax[1, 1].imshow(stimulation_square.detach().cpu().numpy(), cmap='gray', vmin=0, vmax=1.0000e-04)\n",
    "        ax[1, 1].set_title('Neural encoded Stimulation')\n",
    "        ax[1, 1].axis('off')\n",
    "\n",
    "        ax[1, 2].axis('off')\n",
    "\n",
    "        ax[1, 3].imshow(stimulation_target_square.cpu().numpy(), cmap='gray', vmin=0, vmax=1.0000e-04)\n",
    "        ax[1, 3].set_title('Target Stimulation')\n",
    "        ax[1, 3].axis('off')\n",
    "\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
